{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68035eff",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy.io as scio\n",
    "from scipy.io import savemat  \n",
    "import scipy.io as sio\n",
    "from sklearn.metrics import r2_score\n",
    "import shap\n",
    "import matplotlib.pyplot as plt\n",
    "from pandas import Series,DataFrame\n",
    "from mealpy import FloatVar,SSA,WOA,AVOA,SRSR,SLO,FOX,SeaHO,PSO\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split, KFold\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01d2c895",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading Data\n",
    "df = pd.read_excel('data.xlsx')\n",
    "df = df.iloc[:, 1:] \n",
    "X = df.drop(['Fu'], axis=1)\n",
    "y = df['Fu']\n",
    "\n",
    "# Divide the training set and the testing set\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "data_train_x=X_train\n",
    "data_train_y=y_train\n",
    "data_test_x=X_test\n",
    "data_test_y=y_test\n",
    "\n",
    "\n",
    "# Standardization\n",
    "X_mean, y_mean = data_train_x.mean(0), data_train_y.mean(0)\n",
    "X_std, y_std = data_train_x.std(0), data_train_y.std(0)\n",
    "\n",
    "data_train_x_nor = (data_train_x - X_mean) / X_std  \n",
    "data_test_x_nor = (data_test_x - X_mean) / X_std\n",
    "\n",
    "data_train_y_nor = (data_train_y - y_mean) / y_std  \n",
    "data_test_y_nor = (data_test_y - y_mean) / y_std\n",
    "\n",
    "data_train_x_nor = data_train_x_nor.values if isinstance(data_train_x_nor, pd.DataFrame) else data_train_x_nor\n",
    "data_train_y_nor = data_train_y_nor.values if isinstance(data_train_y_nor, pd.DataFrame) else data_train_y_nor\n",
    "\n",
    "data_train_y_nor = data_train_y_nor.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d807925",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation Function\n",
    "\n",
    "def evaluate_regress(y_pre, y_true):\n",
    "   \n",
    "    MAE=np.sum(np.abs(y_pre-y_true))/len(y_true)\n",
    "    print('MAE为: ',str(MAE))\n",
    "\n",
    "    MAPE=np.sum(np.abs((y_pre-y_true)/y_true))/len(y_true)\n",
    "    print('MAPE为: ',str(MAPE))\n",
    "\n",
    "    MSE=np.sum((y_pre-y_true) ** 2)/len(y_true)\n",
    "    print('MSE为: ',str(MSE))\n",
    "    \n",
    "    RMSE=np.sqrt(MSE)\n",
    "    print('RMSE为: ',str(RMSE))\n",
    "\n",
    "    R2=r2_score(y_true, y_pre)\n",
    "    print('R2为: ',str(R2))\n",
    "\n",
    "    return MAE,MAPE,MSE,RMSE,R2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "505b277d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from xgboost import XGBRegressor\n",
    "\n",
    "# Record the start time of the whole process.\n",
    "overall_start_time = time.time()\n",
    "\n",
    "class XGBoostRegressorWrapper:\n",
    "    def __init__(self, \n",
    "                 n_estimators=100,       \n",
    "                 max_depth=6,            \n",
    "                 learning_rate=0.1,      \n",
    "                 min_child_weight=1,     \n",
    "                 gamma=0,                \n",
    "                 subsample=1,            \n",
    "                 colsample_bytree=1,     \n",
    "                 alpha=0,                \n",
    "                 lambd=1,                \n",
    "                 random_state=42,        \n",
    "                 n_jobs=-1               \n",
    "                ):\n",
    "        self.model = XGBRegressor(\n",
    "            n_estimators=n_estimators,\n",
    "            max_depth=max_depth,\n",
    "            learning_rate=learning_rate,\n",
    "            min_child_weight=min_child_weight,\n",
    "            gamma=gamma,\n",
    "            subsample=subsample,\n",
    "            colsample_bytree=colsample_bytree,\n",
    "            reg_alpha=alpha,             \n",
    "            reg_lambda=lambd,            \n",
    "            random_state=random_state,\n",
    "            n_jobs=n_jobs\n",
    "        )\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        self.model.fit(X, y)\n",
    "    \n",
    "    def predict(self, X):\n",
    "        return self.model.predict(X)\n",
    "\n",
    "# Create a global normalization object\n",
    "scaler_x = StandardScaler()\n",
    "scaler_y = StandardScaler()\n",
    "\n",
    "best_fitness_history = []\n",
    "best_params_history = []\n",
    "\n",
    "def evaluate_model(solution):\n",
    "    n_estimators = int(solution[0])\n",
    "    max_depth = int(solution[1])\n",
    "    learning_rate = solution[2]\n",
    "    min_child_weight = int(solution[3])\n",
    "    alpha = solution[4]\n",
    "    lambd = solution[5]\n",
    "    \n",
    "    kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "    mae_scores = []\n",
    "    \n",
    "    for train_index, val_index in kf.split(data_train_x):\n",
    "        X_train_fold, X_val_fold = data_train_x.iloc[train_index], data_train_x.iloc[val_index]\n",
    "        y_train_fold, y_val_fold = data_train_y.iloc[train_index], data_train_y.iloc[val_index]\n",
    "\n",
    "        # Check for NaN values and handle them\n",
    "        if X_train_fold.isnull().values.any() or X_val_fold.isnull().values.any() or y_train_fold.isnull().values.any() or y_val_fold.isnull().values.any():\n",
    "            print(\"NaN values found in training or validation fold. Skipping this fold.\")\n",
    "            continue  \n",
    "        \n",
    "        # Standardization\n",
    "        X_train_fold = scaler_x.fit_transform(X_train_fold)\n",
    "        X_val_fold = scaler_x.transform(X_val_fold)\n",
    "\n",
    "        y_train_fold = scaler_y.fit_transform(y_train_fold.values.reshape(-1, 1)).flatten()\n",
    "        y_val_fold = scaler_y.transform(y_val_fold.values.reshape(-1, 1)).flatten()\n",
    "\n",
    "        model = XGBoostRegressorWrapper(\n",
    "            n_estimators=n_estimators,\n",
    "            max_depth=max_depth,\n",
    "            learning_rate=learning_rate,\n",
    "            min_child_weight=min_child_weight,\n",
    "            gamma=0.001,\n",
    "            subsample=0.8,\n",
    "            colsample_bytree=1,\n",
    "            alpha=alpha,             \n",
    "            lambd=lambd,\n",
    "            random_state=42,\n",
    "            n_jobs=-1            \n",
    "        )\n",
    "        \n",
    "        model.fit(X_train_fold, y_train_fold)\n",
    "        y_pred = model.predict(X_val_fold)\n",
    "\n",
    "        mae_fold = mean_absolute_error(y_val_fold, y_pred)\n",
    "        mae_scores.append(mae_fold)\n",
    "    \n",
    "    if mae_scores:  \n",
    "        mean_mae = np.mean(mae_scores)\n",
    "        \n",
    "        # Update best fitness and corresponding hyperparameters\n",
    "        if len(best_fitness_history) < 150 or mean_mae < best_fitness_history[-1]:\n",
    "            best_fitness_history.append(mean_mae)\n",
    "            best_params_history.append(solution)\n",
    "\n",
    "        if len(best_fitness_history) > 150:\n",
    "            best_fitness_history.pop(0)\n",
    "            best_params_history.pop(0)\n",
    "\n",
    "        return mean_mae\n",
    "    else:\n",
    "        return float('inf') \n",
    "    \n",
    "# Define parameter ranges\n",
    "param_grid = {\n",
    "    \"obj_func\": evaluate_model,\n",
    "    \"bounds\": [\n",
    "        FloatVar(lb=10, ub=200),   \n",
    "        FloatVar(lb=1, ub=5),     \n",
    "        FloatVar(lb=0.01, ub=0.3),\n",
    "        FloatVar(lb=2, ub=20),     \n",
    "        FloatVar(lb=0, ub=1),  \n",
    "        FloatVar(lb=1, ub=15)      \n",
    "    ],\n",
    "    \"minmax\": \"min\"\n",
    "}\n",
    "\n",
    "# Set parameters for the SRSR algorithm\n",
    "epoch = 2\n",
    "pop_size = 15\n",
    "SRSR_model = SRSR.OriginalSRSR(epoch=epoch, pop_size=pop_size)\n",
    "\n",
    "# Solve the optimization problem\n",
    "SRSR_best = SRSR_model.solve(param_grid)\n",
    "\n",
    "# Create a DataFrame to save the best fitness and hyperparameters\n",
    "best_history_df = pd.DataFrame({\n",
    "    'Best_Fitness': best_fitness_history,\n",
    "    'N_Estimators': [int(param[0]) for param in best_params_history],\n",
    "    'Max_Depth': [int(param[1]) for param in best_params_history],\n",
    "    'Learning_Rate': [param[2] for param in best_params_history],\n",
    "    'Min_Child_Weight': [int(param[3]) for param in best_params_history],\n",
    "    'Alpha': [param[4] for param in best_params_history],\n",
    "    'Lambda': [param[5] for param in best_params_history]\n",
    "})\n",
    "\n",
    "# Best parameters\n",
    "final_best_params = SRSR_best.solution\n",
    "\n",
    "# Save DataFrame to Excel\n",
    "output_path = 'optimized_paras_SRSR_XGBoost.xlsx'\n",
    "with pd.ExcelWriter(output_path) as writer:\n",
    "    best_history_df.to_excel(writer, sheet_name='Best_Parameters', index=False)\n",
    "    pd.DataFrame([final_best_params], columns=[f'Final_Param_{i+1}' for i in range(len(final_best_params))]).to_excel(writer, sheet_name='Final_Best_Parameters', index=False)\n",
    "\n",
    "# Retrieve the best parameters\n",
    "best_n_estimators = int(final_best_params[0])\n",
    "best_max_depth = int(final_best_params[1])\n",
    "best_learning_rate = final_best_params[2]\n",
    "best_min_child_weight = int(final_best_params[3])\n",
    "best_alpha = final_best_params[4]\n",
    "best_lambda = final_best_params[5]\n",
    "\n",
    "# Train the final model using the best parameters\n",
    "best_model = XGBoostRegressorWrapper(\n",
    "    n_estimators=best_n_estimators,\n",
    "    max_depth=best_max_depth,\n",
    "    learning_rate=best_learning_rate,\n",
    "    min_child_weight=best_min_child_weight,\n",
    "    gamma=0.001,\n",
    "    subsample=0.8,\n",
    "    colsample_bytree=1,\n",
    "    alpha=best_alpha,             \n",
    "    lambd=best_lambda,\n",
    "    random_state=42,\n",
    "    n_jobs=-1 \n",
    ")\n",
    "best_model.fit(data_train_x_nor, data_train_y_nor)\n",
    "\n",
    "# Prediction\n",
    "y_pred_test_nor = best_model.predict(data_test_x_nor)\n",
    "y_pred_train_nor = best_model.predict(data_train_x_nor)\n",
    "\n",
    "# Anti-standardization\n",
    "y_pred_test = y_pred_test_nor * y_std + y_mean\n",
    "y_pred_test1 = y_pred_test.reshape(len(y_pred_test), 1)\n",
    "data_test_y1 = data_test_y.to_numpy().reshape(len(data_test_y), 1)\n",
    "\n",
    "y_pred_train = y_pred_train_nor * y_std + y_mean\n",
    "y_pred_train1 = y_pred_train.reshape(len(y_pred_train), 1)\n",
    "data_train_y1 = data_train_y.to_numpy().reshape(len(data_train_y), 1)\n",
    "\n",
    "# Calculation error\n",
    "T_MAE, T_MAPE, T_MSE, T_RMSE, T_R2 = evaluate_regress(y_pred_test1, data_test_y1)\n",
    "R_MAE, R_MAPE, R_MSE, R_RMSE, R_R2 = evaluate_regress(y_pred_train1, data_train_y1)\n",
    "\n",
    "# Create a DataFrame\n",
    "errors_test = pd.DataFrame({\n",
    "    'test—Metric': ['MAE', 'MAPE', 'MSE', 'RMSE', 'R2'],\n",
    "    'test—Value': [T_MAE, T_MAPE, T_MSE, T_RMSE, T_R2]\n",
    "})\n",
    "errors_train = pd.DataFrame({\n",
    "    'train—Metric': ['MAE', 'MAPE', 'MSE', 'RMSE', 'R2'],\n",
    "    'train—Value': [R_MAE, R_MAPE, R_MSE, R_RMSE, R_R2]\n",
    "})\n",
    "\n",
    "# Reconstruct predictions and true values\n",
    "predictions = np.concatenate((y_pred_train[:, np.newaxis], y_pred_test[:, np.newaxis]), axis=0)\n",
    "truevalues = np.concatenate((data_train_y.to_numpy()[:, np.newaxis], data_test_y.to_numpy()[:, np.newaxis]), axis=0)\n",
    "predictions = predictions.ravel()\n",
    "truevalues = truevalues.ravel()\n",
    "\n",
    "results_df = pd.DataFrame({'Predictions': predictions, 'True Values': truevalues})\n",
    "\n",
    "# Save the results to Excel\n",
    "output_path_results = 'results_optimized_SRSR_XGBoost.xlsx'\n",
    "with pd.ExcelWriter(output_path_results) as writer:\n",
    "    errors_test.to_excel(writer, sheet_name='Test_Errors', index=False)\n",
    "    errors_train.to_excel(writer, sheet_name='Train_Errors', index=False)\n",
    "    results_df.to_excel(writer, sheet_name='Predictions', index=False)\n",
    "\n",
    "# Record the end time of the whole process.\n",
    "overall_end_time = time.time()\n",
    "overall_total_time = overall_end_time - overall_start_time\n",
    "print(f\"Total time taken: {overall_end_time - overall_start_time:.2f} seconds\")\n",
    "\n",
    "\n",
    "# Save total runtime to an Excel file\n",
    "with pd.ExcelWriter(output_path, engine='openpyxl', mode='a') as writer:\n",
    "    process_time_df = pd.DataFrame({\n",
    "        'Total_Process_Time_Seconds': [overall_total_time]\n",
    "    })\n",
    "    process_time_df.to_excel(writer, sheet_name='Process_Time', index=False)\n",
    "\n",
    "print(f'Save to: {output_path} excel ')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c19eff9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DL",
   "language": "python",
   "name": "dl"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
